# -*- coding: utf-8 -*-
"""Email_Classifier_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BUvlihCILj7a4SPtgcsSV_MZ9or4aNyr

## Data Loading & Preprocessing

### Step 1: Load Raw Email Dataset
- Import the email data from a CSV file (`emails.csv`).
- Use proper encoding (`latin1`) and skip bad lines to prevent read errors.
- Limit the number of rows to 8000 for manageable size during development.

### Step 2: Extract Email Body
- Raw emails often contain metadata and headers.
- Use a custom function to extract only the main message content.
- Store the clean text in a new column: `email_body`.

---

## Data Augmentation

### Add Sample Promotional Emails
- Manually written examples of promotions, sales, and discount emails.
- Includes offers like "50% OFF", "BOGO", "Flash Sale", etc.
- Helps balance the Promotions category which may be underrepresented.

### Add Sample Finance Emails
- Include common emails from banks and corporate finance teams:
  - Payment confirmations
  - Account summaries
  - Budget approvals
  - Reimbursements
- Improves the model's ability to classify formal financial communications.

### Add Sample Billing Emails
- Examples of monthly bills and service-related invoices:
  - Electricity, water, rent, internet, subscriptions
  - Payment due reminders and late fee notices
- Strengthens the model's detection of recurring utility messages.
## **NOTE**: As we have selected Enron Email dataset, it had very few examples regarding some labels so I augmented data so that the classifier could perform better on these labels.
---

## Final Dataset Merging

### Concatenate All Data Sources
- Combine:
  - Cleaned original email dataset
  - Manually created promotional, finance, and bills emails
  - Additional pre-prepared data from `extra_emails.csv`
- Result: a more balanced, diverse, and robust dataset for training the classifier.
"""

import pandas as pd
import numpy as np

data = pd.read_csv('/content/emails.csv',encoding='latin1',engine='python',on_bad_lines='skip',nrows=8000)
print(data.shape)
data.head(5)

def extract_email_body(text):
    try:
        return text.split("\n\n",1)[1]  #remove headers
    except:
        return ""

data['email_body']=data['message'].apply(extract_email_body)
print(data.head(5))

#adding some sample emails
new_emails = [

    #Promotions
    "Exclusive Deal! Use code SAVE30 to get 30% off your next purchase.",
    "Flash Sale: Everything 60% off today only!",
    "Buy one, get one free on all accessories this weekend.",
    "Your special coupon code is here: APPLY20 for 20% off!",
    "Limited Time Offer: Shop now and enjoy 40% off sitewide!",
    "Mega Sale! Grab your favorite items with up to 70% discount.",
    "Hurry! 2 days left to claim your 25% off voucher.",
    "This week only: Save big on electronics with code TECH10.",
    "End of season sale: Huge discounts on all fashion items.",
    "Hot Deal Alert: Get 50% off your favorite brands now.",
    "Special Promo: Free shipping + 15% off all new arrivals.",
    "Weekend Only: Save ₹500 on purchases over ₹2500.",
    "Loyalty Bonus: Use THANKYOU10 to get an extra 10% off.",
    "Just for You: Flat 40% off on your next shopping spree!",
    "Biggest Sale of the Year! Don't miss up to 80% off.",
    "Pre-launch Offer: Reserve now and save 20%.",
    "Early Access: Get 15% off before the public launch!",
    "Back to School Offer: Buy 2 stationery items, get 1 free.",
    "Use PROMO50 at checkout and enjoy 50% savings!",
    "Clearance Sale: Everything must go – up to 70% off.",
    "Daily Deal: One-time 25% off – today only!",
    "Celebrate Diwali with us: Get 30% off on festive wear.",
    "Cyber Monday: Exclusive 40% discount on all tech gadgets.",
    "New Year Bonanza: Free gifts + extra 10% off storewide.",
    "Don't miss our limited 24-hour deal – 55% off shoes.",
    "Spring Sale: Fresh styles now 35% off!",
    "Final Call! Grab your discount before midnight.",
    "Your reward: 20% off everything in your cart.",
    "Student Discount: Flat ₹100 off on course enrollments.",
    "App-only Deal: Download and get ₹200 off instantly.",
    "Thanksgiving Offer: Free dessert on orders over ₹500.",
    "Giveaway: Enter now to win ₹1000 gift card!",
    "EXTRA10: Use this code to enjoy extra savings today.",
    "Valentine's Special: Buy gift items at 40% off.",
    "Bestseller Deal: Top picks now under ₹999.",
    "Limited Stocks: Reserve yours now and save 25%.",
    "Today's Top Offer: Everything ₹199 only!",
    "Flat 60% off + Free delivery on all prepaid orders.",
    "Invite your friends & get ₹300 cashback each!",
    "Special Offer Just For You: Use code SECRET15 at checkout."
]

new_df = pd.DataFrame(new_emails, columns=["email_body"])
data = pd.concat([data, new_df], ignore_index=True)

finance_emails = [
    {"email_body": "We’ve received your payment. Thank you."},
    {"email_body": "Your account summary for June is attached."},
    {"email_body": "Annual budget has been finalized by the CFO."},
    {"email_body": "Kindly approve the finance team’s travel expenses."},
    {"email_body": "Your account will be reviewed for compliance."},
    {"email_body": "The finance department has issued a new circular."},
    {"email_body": "You are eligible for expense reimbursement."},
    {"email_body": "Please send receipts for the previous claims."},
    {"email_body": "Your investment statement is now available."},
    {"email_body": "Bank transfer completed successfully."},
    {"email_body": "Audit preparation documents are due tomorrow."},
    {"email_body": "Payment pending for vendor services."},
    {"email_body": "Budget approval meeting is scheduled for 2 PM."},
    {"email_body": "Transaction history has been emailed."},
    {"email_body": "Monthly balance report ready for review."},
    {"email_body": "Please categorize all spending for Q2."},
    {"email_body": "Expense policy has been updated."},
    {"email_body": "All invoices must be verified before submission."},
    {"email_body": "Submit your expense reports via the new portal."},
    {"email_body": "Tax-related documents are to be filed by next week."},
    {"email_body": "Finance team will be unavailable due to audit."},
    {"email_body": "Reminder: Salary disbursement is this Friday."},
    {"email_body": "Details of the company’s financial projections are attached."},
    {"email_body": "We are streamlining petty cash operations."},
    {"email_body": "Update your bank info to avoid payment delays."},
    {"email_body": "Credit card reconciliation report is pending."},
    {"email_body": "Attach all receipts to the travel expense form."},
    {"email_body": "We’ve migrated to a new accounting software."},
    {"email_body": "Check your reimbursement eligibility via HRMS."},
    {"email_body": "Bonus calculations will be shared next week."}
]
bills_emails = [
    {"email_body": "Your phone bill for August is now available."},
    {"email_body": "Electricity usage has crossed the monthly limit."},
    {"email_body": "New billing cycle starts from 1st August."},
    {"email_body": "Water utility payment is due in 2 days."},
    {"email_body": "Attached is your monthly rent receipt."},
    {"email_body": "Cable bill payment confirmation enclosed."},
    {"email_body": "Kindly clear your outstanding balance for gas supply."},
    {"email_body": "Your monthly Netflix invoice is attached."},
    {"email_body": "Amazon Prime subscription renewal notice."},
    {"email_body": "Overdue electricity bill – pay now to avoid disconnection."},
    {"email_body": "Broadband service alert: payment overdue."},
    {"email_body": "You are eligible for an electricity bill discount."},
    {"email_body": "Mobile plan has been renewed automatically."},
    {"email_body": "Final reminder: Water bill for May is unpaid."},
    {"email_body": "Digital invoice generated successfully."},
    {"email_body": "Pay your sewerage bill online and save ₹50."},
    {"email_body": "Insurance installment due by 30th July."},
    {"email_body": "New statement generated: Auto loan payment."},
    {"email_body": "TV subscription plan updated as per your request."},
    {"email_body": "Electricity unit details and cost breakdown attached."},
    {"email_body": "Meter reading report for your flat is ready."},
    {"email_body": "Home maintenance fee for Q3 is now due."},
    {"email_body": "Clear your storage unit dues before 15th Aug."},
    {"email_body": "Final rent invoice before lease termination."},
    {"email_body": "Late fees will apply from next billing cycle."},
    {"email_body": "Kindly verify the gas bill attached."},
    {"email_body": "Your apartment's service charges are overdue."},
    {"email_body": "Automated receipt for garbage collection service."},
    {"email_body": "Monthly heating bill ready for download."},
    {"email_body": "Reminder: Electricity bill auto-debit scheduled for 5th Aug."}
]
data = pd.concat([data, pd.DataFrame(finance_emails + bills_emails)], ignore_index=True)

extra_data = pd.read_csv("extra_emails.csv")#reading from the file
data = pd.concat([data, extra_data], ignore_index=True)

data.tail(20)

"""## Text Cleaning & Preprocessing

### Libraries Used:
- `BeautifulSoup`: To parse and strip HTML content.
- `re` (Regular Expressions): To clean unwanted patterns like URLs, emails, and punctuation.
- `nltk`: For handling stopwords and general NLP utilities.

---

### Cleaning Pipeline Description

This function processes raw email bodies and transforms them into clean, model-friendly text.

#### Steps Performed:

- **HTML Tag Removal**
  - Strips any embedded HTML using `BeautifulSoup`.

- **Lowercasing**
  - Converts all characters to lowercase to ensure uniformity.

- **Removal of URLs and Email Addresses**
  - Removes web links (e.g., `https://...`) and email IDs (e.g., `abc@example.com`) to reduce noise.

- **Punctuation, Numbers, and Special Characters**
  - Filters out all non-alphabetic characters, retaining only words.

- **Whitespace Normalization**
  - Collapses multiple spaces into a single space for consistency.

- **Stopword Removal**
  - Removes common English words (e.g., *the*, *is*, *and*) using NLTK’s stopword list to reduce noise in training.

---

### Output
- A new column `cleaned_text` is added to the dataset.
- It contains the cleaned version of each email, ready for vectorization and model training.

"""

import re
from bs4 import BeautifulSoup
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

#load english language stopwords
stop_words=set(stopwords.words('english'))

def clean_email(text):
    #1.Remove HTML tags
    text = BeautifulSoup(text,"html.parser").get_text()

    #2.Lowercase
    text = text.lower()

    #3.Remove URLs and email addresses
    text = re.sub(r'http\S+|www\S+', '',text)
    text = re.sub(r'\S+@\S+', '',text)

    #4.Remove punctuation, numbers, and special characters
    text = re.sub(r'[^a-z\s]', '',text)

    #5.Normalize whitespace
    text = re.sub(r'\s+', ' ',text)

    #6.Remove stopwords
    text = ' '.join(word for word in text.split() if word not in stop_words)

    return text.strip()



data['cleaned_text']=data['email_body'].apply(clean_email)
data[['email_body', 'cleaned_text']].tail(5)

#Creating a cleaned dataset file-just for good practice!
data.to_csv("enron_cleaned.csv", index=False)

#Sample output to verify how cleaning is done
i = 100
print("FULL MESSAGE:\n", data.loc[i, 'message'])
print("\nEXTRACTED BODY:\n", data.loc[i, 'email_body'])
print("\nCLEANED TEXT:\n", data.loc[i, 'cleaned_text'])

"""## Lemmatization & Linguistic Preprocessing (spaCy)

### Libraries Used:
- `spaCy`: For advanced NLP processing including:
  - Lemmatization
  - Part-of-speech tagging
  - Token filtering (stopwords, punctuation, emails, URLs)

---

### Objective
To enhance the cleaned email text by applying **linguistic normalization** using spaCy. This prepares the data for better feature extraction and classification performance.

---

### Steps Performed:

- **Load spaCy Model**
  - The lightweight English model `en_core_web_sm` is loaded, which supports lemmatization, POS tagging, and entity recognition.

- **Batch Processing**
  - All pre-cleaned email texts (`cleaned_text`) are processed in **batches** using `nlp.pipe()` for faster performance compared to individual loops.

- **Lemmatization & Filtering**
  - For each token in the document:
    - Convert to its **base/root form** (lemma)
    - Convert to lowercase
    - Exclude:
      - Stopwords (e.g., "the", "is")
      - Punctuation
      - Email addresses
      - URLs

- **New Column Added**
  - `lemmatized_text`: Contains the final lemmatized and linguistically cleaned version of each email.

---

### Output
- The dataset now includes a new column:
  -  `lemmatized_text` — refined, language-aware tokens for model input.
- This version is better suited for feature extraction (e.g., TF-IDF, embeddings).

```python
data[['cleaned_text','lemmatized_text']].head(5)
```


"""

import spacy
import pandas as pd

#load English spaCy model with POS tagging & lemmatization enabled
nlp = spacy.load("en_core_web_sm")

#function to clean a spaCy Doc object
def get_cleaned(doc):
    return ' '.join([
        token.lemma_.lower().strip()
        for token in doc
        if not token.is_stop and not token.is_punct and not token.like_email and not token.like_url
    ])

#convert email bodies to list for batch processing-enables faster processing when working in batches
texts = data['cleaned_text'].tolist()

#process all texts in batch (efficient and faster)
docs = list(nlp.pipe(texts, batch_size=1000))

#apply cleaning to each processed doc
data['lemmatized_text'] = [get_cleaned(doc) for doc in docs]

print(data[['cleaned_text', 'lemmatized_text']].head(5))

"""## Feature Extraction, Labeling & Model Training

### 1. TF-IDF Vectorization

- **Library Used**: `TfidfVectorizer` from `sklearn`
- **Purpose**: Convert text data into numerical vectors using the **Term Frequency-Inverse Document Frequency (TF-IDF)** technique.
- **Key Action**:
  - Uses the `lemmatized_text` column to compute a document-term matrix.
  - Limits the number of features to the **top 5000** most relevant tokens.
- **Output**:
  - A sparse matrix `X` of shape `(n_emails, 5000)` representing the dataset numerically.
  - A list of top feature names for exploration.

---

### 2. Rule-Based Label Assignment

- **Function: `assign_label()`**
- **Goal**: Assign labels (like `HR`, `Bills`, `Work`, etc.) to unlabeled emails based on **keyword matching**.
- **Categories & Keywords**:
  - **Bills**: invoice, due date, utility, overdue...
  - **HR**: job, interview, hiring, CV...
  - **Promotions**: offer, sale, discount, coupon...
  - **Work**: meeting, task, report, deadline...
  - **Finance**: bank, statement, transaction...
- **Fallback**: Emails not matching any specific category are labeled as `"Work"` by default.
- **Output**:
  - A new column `label` added to the DataFrame with assigned categories.

---

### 3. Train-Test Split

- **Library Used**: `train_test_split` from `sklearn.model_selection`
- **Purpose**: Split the TF-IDF features and assigned labels into **training and testing sets**.
- **Parameters**:
  - `test_size = 0.2` → 20% of the data for testing
  - `stratify = data['label']` → ensures balanced category distribution
- **Output**:
  - `X_train`, `X_test`, `y_train`, `y_test`

---

### 4. Model Training — Logistic Regression

- **Model**: `LogisticRegression` with `max_iter=1000`
- **Purpose**: Train a **multiclass classifier** using the TF-IDF features.
- **Training**: Fit on the training set (`X_train`, `y_train`)
- **Algorithm Type**: Linear classifier suited for text classification problems.

---

### 5. Evaluation Metrics

- **Predictions**: Made on the test set (`X_test`)
- **Metrics Used**:
  - **Accuracy**: Overall correctness
  - **Classification Report**: Includes precision, recall, f1-score for each class

- **Purpose**: Measure how well the model performs on unseen data and assess **category-wise effectiveness**.

---



"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf=TfidfVectorizer(max_features=5000) #max features can be adjusted by your choice
X = tfidf.fit_transform(data['lemmatized_text'])

print("TF-IDF matrix shape:", X.shape)

feature_names=tfidf.get_feature_names_out()
print("Sample features:",feature_names[:20])

#assigning labels to our emails

def assign_label(text):
    text = text.lower()
    if any(keyword in text for keyword in [
        'invoice', 'payment due', 'amount due', 'bill', 'outstanding', 'due date', 'pay now', 'utility', 'overdue'
    ]):
        return 'Bills'

    elif any(keyword in text for keyword in [
        'job', 'interview', 'resume', 'position', 'recruit', 'career', 'cv', 'hiring', 'apply now', 'vacancy'
    ]):
        return 'HR'

    elif any(keyword in text for keyword in [
        'offer', 'sale', 'discount', 'deal', 'limited time', 'exclusive', 'coupon', 'save', 'promo', 'flash sale',
        'bogo', 'free shipping', 'clearance', 'lowest price'
    ]):
        return 'Promotions'

    elif any(keyword in text for keyword in [
        'meeting', 'project', 'deadline', 'task', 'update', 'follow up', 'deliverable', 'brief', 'report', 'assign'
    ]):
        return 'Work'

    elif any(keyword in text for keyword in [
        'account', 'transaction', 'bank', 'statement', 'balance', 'debit', 'credit card', 'atm', 'fund transfer'
    ]):
        return 'Finance'

    else:
        return 'Work'  # Fallback

data['label'] = data['email_body'].apply(assign_label)

from sklearn.model_selection import train_test_split

X_train,X_test, y_train, y_test=train_test_split(
    X, data['label'], test_size=0.2, random_state=42, stratify=data['label']
)

from sklearn.linear_model import LogisticRegression

model=LogisticRegression(max_iter=1000)
model.fit(X_train,y_train)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test,y_pred))
print("\nClassification Report:\n", classification_report(y_test,y_pred))

"""## Export Model & Vectorizer"""

import joblib

joblib.dump(model, "email_classifier_model.pkl")
joblib.dump(tfidf, "tfidf_vectorizer.pkl")